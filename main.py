import pandas as pd
import pdfplumber
import os
import requests
import re
import xml.etree.ElementTree as ET
import sys
import codecs

import jageocoder
from enum import Enum
from datetime import datetime

from urllib.parse import urlparse

# ç·¯åº¦çµŒåº¦ãƒ€ãƒ–ãƒ«ãƒã‚§ãƒƒã‚¯
import geopandas as gpd
from shapely.geometry import Point
# åŠè§’å¤‰æ›
import unicodedata
# ãƒ­ã‚°å‡ºåŠ›
import logging
from logging import getLogger, basicConfig, INFO

# ==================================================================
# Function
# ==================================================================


def load_prefectures(csv_file):
    """
    éƒ½é“åºœçœŒç•ªå·ã¨åå‰ã®å¯¾å¿œè¾æ›¸ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€
    """
    df = pd.read_csv(csv_file, dtype={
                     'number': int, 'name': str, 'english_name': str})
    return {row['english_name']: row['name'] for _, row in df.iterrows()}, list(df['english_name'])


def get_prefecture_name(prefecture_english_name):
    """
    éƒ½é“åºœçœŒã®è‹±èªåã‹ã‚‰æ—¥æœ¬èªåã‚’å–å¾—
    """
    return prefectures.get(prefecture_english_name, "")


def address_to_coordinates(address):
    """
    ä½æ‰€ã‹ã‚‰ç·¯åº¦çµŒåº¦ã‚’å–å¾—
    note: (token)ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯jageocoder_searchã«åˆ‡ã‚Šæ›¿ãˆã‚‹
    """
    if not address:
        return 0, 0
    try:
        base_url = "http://geocode.csis.u-tokyo.ac.jp/cgi-bin/simple_geocode.cgi?charset=UTF8&addr="
        url = base_url + requests.utils.quote(str(address))
        latitude, longitude = 0, 0
        response = requests.get(url)
        if response.status_code == 200:
            xml_content = response.text
            xml_content = xml_content.replace("\n", "")
            root = ET.fromstring(xml_content)

            # å°æ•°ç‚¹ä»¥ä¸‹ç¬¬6ä½ã¾ã§å–å¾—
            longitude = round(
                float(root.findtext(".//candidate/longitude")), 6)
            latitude = round(float(root.findtext(".//candidate/latitude")), 6)

    except Exception as e:
        logger.error(f"{address_to_coordinates} {e}")
        latitude, longitude = jageocoder_search(address)

    return latitude, longitude


def jageocoder_search(address):
    """
    ä½æ‰€ã‹ã‚‰ç·¯åº¦çµŒåº¦ã‚’å–å¾— (jageocoderã‚’ä½¿ç”¨)
    address_to_coordinatesãŒã‚¨ãƒ©ãƒ¼ã®å ´åˆã«ä½¿ç”¨
    """
    if not address:
        return 0, 0

    address = str(address)
    result = jageocoder.search(address)

    if result['candidates']:
        # æœ€åˆã®å€™è£œã‹ã‚‰ç·¯åº¦çµŒåº¦ã‚’å–å¾—
        latitude = result['candidates'][0]['y']
        longitude = result['candidates'][0]['x']

        # ç·¯åº¦çµŒåº¦ã®ç¯„å›²ã‚’ç¢ºèªã™ã‚‹
        if (-90 <= latitude <= 90) and (-180 <= longitude <= 180):
            return round(latitude, 6), round(longitude, 6)

    return 0, 0


def split_japanese_address(address):
    """
    ä½æ‰€ã‚’éƒ½é“åºœçœŒã€å¸‚åŒºç”ºæ‘ã€ãã‚Œä»¥é™ã«åˆ†å‰²
    """
    if not address:
        return ["", ""]

    pattern = re.compile(
        r'(?:(?P<region>...??[éƒ½é“åºœçœŒ]))?'  # éƒ½é“åºœçœŒ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        r'(?P<locality>.+?[å¸‚åŒºç”ºæ‘æ¹¾å³¶])'  # å¸‚åŒºç”ºæ‘ãªã©
        r'(?P<remainder>.*)'  # ãã‚Œä»¥é™ã®ä½æ‰€
    )

    match = pattern.match(address)
    if match:
        result = match.groupdict()
        region = result['region'] if result['region'] else ""
        locality = result['locality'] if result['locality'] else ""

        return [region, locality]
    else:
        return ["", ""]


def postal2location(postal_code):
    """
    éƒµä¾¿ç•ªå·ã‹ã‚‰å¸‚åŒºç”ºæ‘åã¨å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
    """
    if pd.isna(postal_code):
        return "", "", ""

    postal_code = postal_code.replace("-", "")

    # 1. ken_all.csvã¨çªãåˆã‚ã›
    prefecture, city = "", ""
    city_code = ""
    for _, row in address_df.iterrows():
        if row["postal"].strip() == postal_code:
            prefecture = row["prefecture"].strip()
            city = row["city"].strip()
            city_code = row["jis"].strip()
            break

    # 2. å€‹åˆ¥äº‹æ¥­æ‰€ãƒ‡ãƒ¼ã‚¿ã¨çªãåˆã‚ã›
    if prefecture == "" and city == "":
        for _, row in jigyosyo_df.iterrows():
            if row["postal"].strip() == postal_code:
                prefecture = row["prefecture"].strip()
                city = row["city"].strip()
                city_code = row["jis"].strip()
                break

    # 5æ¡ã‚’6æ¡ã«å¤‰æ›
    if city_code != "":
        tmp_code = f'{city_code:05}'  # 0åŸ‹ã‚ã§5æ–‡å­—
        city_code = convert_five_to_six_digit_code(tmp_code)

    return prefecture, city, city_code


def is_valid_url(url):
    if not urlparse(url).scheme:
        url = 'https://' + url
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except ValueError:
        return False


def is_accessible_url(url):
    if not urlparse(url).scheme:
        url = 'https://' + url
    try:
        response = requests.head(url, allow_redirects=True, timeout=5)
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒ200ç•ªå°ã¾ãŸã¯300ç•ªå°ã§ã‚ã‚Œã°ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã¨åˆ¤æ–­
        return response.status_code < 400
    except requests.RequestException:
        return False


def delete_title(df):
    """
    å¤§åˆ†çœŒã«ä¸è¦ãªã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹ãŸã‚å‰Šé™¤
    """
    if df.iloc[0, 0] == "ç·Šæ€¥é¿å¦Šã«ä¿‚ã‚‹è¨ºç™‚ãŒå¯èƒ½ãªç”£å©¦äººç§‘åŒ»ç™‚æ©Ÿé–¢ç­‰ä¸€è¦§":
        return df.drop(df.index[:1])
    return df


def delete_headers(df, line_number):
    """
    ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’å‰Šé™¤
    """
    target_list = ["åŸºæœ¬æƒ…å ±", "æ–½è¨­å", "åŒ»ç™‚æ©Ÿé–¢å"]
    for target in target_list:
        if df.iloc[0, 0] == target or (len(df.columns) > 1 and df.iloc[0, 1] == target):
            df = df.drop(df.index[:line_number])
    return df


def fix_format_page_df(df, line_number):
    """
    ãƒšãƒ¼ã‚¸ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä¿®æ­£
    """
    return delete_headers(delete_title(df), line_number)


def zenkaku_to_hankaku_regex(text):
    """
    å…¨è§’ã‚’åŠè§’ã«å¤‰æ›ã™ã‚‹é–¢æ•°
    """
    if text:
        text = unicodedata.normalize('NFKC', text)
    return text


def clear_change_line(df):
    """
    è¡Œã®è¡¨è¨˜ã‚’çµ±ä¸€ã™ã‚‹å‡¦ç†
    """
    # æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤
    df[['æ–½è¨­_åç§°', 'é€£çµ¡å…ˆ_é›»è©±ç•ªå·', 'é€£çµ¡å…ˆ_FormURL', 'é€£çµ¡å…ˆ_éƒµä¾¿ç•ªå·', 'é€£çµ¡å…ˆ_ä½æ‰€', 'ç·Šæ€¥é¿å¦Šã«ä¿‚ã‚‹è¨ºç™‚ãŒå¯èƒ½ãªç”£å©¦äººç§‘åŒ»ç™‚æ©Ÿé–¢ç­‰_ç”£ç§‘ã€å©¦äººç§‘åˆã¯ç”£å©¦äººç§‘ã®æ¨™æ¦œã®æœ‰ç„¡', 'ç·Šæ€¥é¿å¦Šã«ä¿‚ã‚‹è¨ºç™‚ãŒå¯èƒ½ãªç”£å©¦äººç§‘åŒ»ç™‚æ©Ÿé–¢ç­‰_å¸¸æ™‚ã®ç·Šæ€¥é¿å¦Šè–¬ã®åœ¨åº«ã®æœ‰ç„¡']].replace(r'\r\n|\r|\n', '', regex=True, inplace=True)

    # "ã‚’å‰Šé™¤
    df.replace('"', '', regex=True, inplace=True)

    # æ™‚é–“è¡¨è¨˜ã®ã€Œ~ã€ã‚’ã€Œ-ã€ã«å¤‰æ›
    hyphens = ['-', 'Ë—', 'á…³', 'á­¸', 'â€', 'â€‘', 'â€’', 'â€“', 'â€”', 'â€•', 'âƒ', 'â»', 'âˆ’', 'â–¬', 'â”€', 'â”', 'â–', 'ãƒ¼', 'ã…¡', 'ï¹˜', 'ï¹£', 'ï¼', 'ï½°', 'ğ„', 'ğ†‘', 'áš€']
    df.replace('~', '-', regex=True, inplace=True)
    df.replace('ã€œ', '-', regex=True, inplace=True)
    df.replace(hyphens, '-', regex=True, inplace=True)

    # å…¨è§’ã‚’åŠè§’ã«å¤‰æ›ã™ã‚‹
    df = df.apply(lambda x: x.map(zenkaku_to_hankaku_regex))

    # ãƒ‡ãƒ¼ã‚¿ãŒ2ã¤æœªæº€ã®è¡Œã¯ä¸è¦ãªå¯èƒ½æ€§ãŒé«˜ã„ã®ã§è¡Œã‚’å‰Šé™¤ & åˆ—åã«æ¬ æå€¤ãŒã‚ã‚‹å ´åˆã‚‚åˆ—ã”ã¨å‰Šé™¤
    df.dropna(axis=0, thresh=2, inplace=True)

    # éƒµä¾¿ç•ªå·ã®æ¬„ã«ã€Œã€’ã€ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤
    df["é€£çµ¡å…ˆ_éƒµä¾¿ç•ªå·"] = df["é€£çµ¡å…ˆ_éƒµä¾¿ç•ªå·"].str.replace("ã€’", "")

    # ä½•ã‚‚ãªã„è¡Œã‚’å‰Šé™¤(ex:é™å²¡çœŒ)
    # åç§°ã€ä½æ‰€ã€éƒµä¾¿ç•ªå·ãŒãªã„è¡Œã§åˆ¤å®šã™ã‚‹
    df.dropna(subset=["æ–½è¨­_åç§°", "é€£çµ¡å…ˆ_ä½æ‰€", "é€£çµ¡å…ˆ_éƒµä¾¿ç•ªå·"], how="all", inplace=True)

    return df


def get_first_page(first_table, prefecture_name):
    """
    æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã«ã€Œå…¬è¡¨ã®å¸Œæœ›ã®æœ‰ç„¡ã€ã‚’è¿½åŠ 
    """
    row = 1

    if prefecture_name == "æ–°æ½ŸçœŒ":
        row = 0

    headers = first_table[row]

    # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒã€ŒåŸºæœ¬æƒ…å ±ã€ã«ãªã£ã¦ã„ã‚‹å ´åˆãŒã‚ã‚‹ã®ã§ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—
    if headers[0] == "åŸºæœ¬æƒ…å ±":
        row += 1
        headers = first_table[row]
    headers = [header.replace('\n', '').replace(
        '\r', '') if header else '' for header in first_table[row]]

    # æ²–ç¸„ã ã‘ãƒ˜ãƒƒãƒ€ãƒ¼ã®æœ€åˆæ¬„ã«ã€Œå…¬è¡¨ã®å¸Œæœ›ã®æœ‰ç„¡ã€ã‚’å…¥ã‚Œã‚‹
    if prefecture_name == "æ²–ç¸„çœŒ":
        headers[0] = "å…¬è¡¨ã®å¸Œæœ›ã®æœ‰ç„¡"

    data = first_table[row+1:]
    return headers, data


def unify_column_names(df, prefecture_name):
    """
    ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã‚«ãƒ©ãƒ ã‚’å¤‰æ›/çµ±ä¸€
    """
    if prefecture_name not in output_format_list_df.columns:
        logger.error(
            f"Error: {prefecture_name} is not in output_format_list_df columns")
        return df

    # å¤‰æ›ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å–å¾—
    column_mapping = output_format_list_df[prefecture_name].dropna().to_dict()

    # ã‚«ãƒ©ãƒ åã‚’å¤‰æ›
    # ä¾‹: åŒ»ç™‚æ©Ÿé–¢ã«ãŠã‘ã‚‹ç·Šæ€¥é¿å¦Šã«ä¿‚ã‚‹å¯¾é¢è¨ºç™‚ã¸ã®å¯¾å¿œå¯èƒ½æ™‚é–“å¸¯ -> åŒ»ç™‚æ©Ÿé–¢ã«ãŠã‘ã‚‹ç·Šæ€¥é¿å¦Šã«ã‹ã‹ã‚‹å¯¾é¢è¨ºç™‚ã¸ã®å¯¾å¿œå¯èƒ½æ™‚é–“å¸¯
    for old_col_index, old_col in column_mapping.items():
        # old_colã¯output_format_list_dfã®nameã®old_col_indexã®å€¤
        new_col = output_format_list_df.loc[old_col_index, 'name']

        if old_col in df.columns:
            df.rename(columns={old_col: new_col}, inplace=True)

    return df


def reorder_columns(df):
    """
    output_format.csvã®nameåˆ—ã«è¨˜è¼‰ã•ã‚ŒãŸé †ç•ªã«dfã®åˆ—ã‚’æ•´ç†
    """
    desired_order = output_format_list_df['name'].dropna().tolist()
    existing_columns = [col for col in desired_order if col in df.columns]
    missing_columns = [col for col in desired_order if col not in df.columns]

    # æ¬ è½ã—ã¦ã„ã‚‹åˆ—ã‚’è¡¨ç¤º
    if missing_columns:
        logger.warning(f"Missing columns: {missing_columns}")

    df = df[existing_columns]

    return df


def calculate_check_digit(five_digit_code):
    # å„æ¡ã«é‡ã¿ã‚’æ›ã‘ã¦åˆè¨ˆã‚’æ±‚ã‚ã‚‹
    weights = [6, 5, 4, 3, 2]  # é‡ã¿
    total = sum(int(digit) * weight for digit,
                weight in zip(five_digit_code, weights))

    # åˆè¨ˆã‚’11ã§å‰²ã£ãŸä½™ã‚Šã‚’æ±‚ã‚ã‚‹
    remainder = total % 11

    # ä½™ã‚Šã‹ã‚‰ãƒã‚§ãƒƒã‚¯ãƒ‡ã‚£ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—
    if remainder == 0:
        check_digit = 0
    else:
        check_digit = 11 - remainder

    # ç‰¹æ®Šã‚±ãƒ¼ã‚¹
    if check_digit == 10:
        check_digit = 0

    return check_digit


def convert_five_to_six_digit_code(five_digit_code):
    check_digit = calculate_check_digit(five_digit_code)
    new_code = f"{five_digit_code}{check_digit}"

    return new_code


def check_location_in_japan(row):
    """
    æŒ‡å®šã•ã‚ŒãŸåº§æ¨™ãŒæŒ‡å®šã•ã‚ŒãŸéƒ½é“åºœçœŒã¨å¸‚åŒºç”ºæ‘ã«å±ã™ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹é–¢æ•°ã€‚

    Parameters:
    row (pandas.Series): ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®1è¡Œ

    Returns:
    str: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆã‚¨ãƒ©ãƒ¼ãŒãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
    """
    latitude = row["ä½æ‰€_ç·¯åº¦"]
    longitude = row["ä½æ‰€_çµŒåº¦"]
    expected_prefecture = row["ä½æ‰€_éƒ½é“åºœçœŒ"]

    # ç·¯åº¦çµŒåº¦ãŒ0ã®å ´åˆã¯ã™ã§ã«ã‚¨ãƒ©ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
    if latitude == 0 or longitude == 0 or row["ä½æ‰€_éƒ½é“åºœçœŒ"] == "":
        return row["ã‚¨ãƒ©ãƒ¼"]

    # æŒ‡å®šã•ã‚ŒãŸåº§æ¨™ã®ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ
    point = Point(longitude, latitude)

    # åº§æ¨™ãŒå«ã¾ã‚Œã‚‹éƒ½é“åºœçœŒã‚’æ¤œç´¢
    matching_area = gdf[gdf.contains(point)]

    if matching_area.empty:
        return add_error_message(row, ERROR_LIST.LAT_LON_MISMATCH.value)

    # æŠ½å‡ºã•ã‚ŒãŸéƒ½é“åºœçœŒ
    extracted_prefecture = matching_area.iloc[0]['N03_001']  # éƒ½é“åºœçœŒå

    # æä¾›ã•ã‚ŒãŸéƒ½é“åºœçœŒã¨æ¯”è¼ƒ
    if extracted_prefecture != expected_prefecture:
        return add_error_message(row, ERROR_LIST.LAT_LON_MISMATCH.value)

    return row["ã‚¨ãƒ©ãƒ¼"]


def add_error_message(row, error_message):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹
    """
    if "ã‚¨ãƒ©ãƒ¼" not in row.keys():
        row["ã‚¨ãƒ©ãƒ¼"] = ""

    row["ã‚¨ãƒ©ãƒ¼"] = f"{row['ã‚¨ãƒ©ãƒ¼']}, {error_message}" if row["ã‚¨ãƒ©ãƒ¼"] else error_message
    logger.error(
        f"[{error_message}] {row['æ–½è¨­_åç§°']}: éƒ½é“åºœçœŒ:{row['ä½æ‰€_éƒ½é“åºœçœŒ']}, å¸‚åŒºç”ºæ‘:{row['ä½æ‰€_å¸‚åŒºç”ºæ‘ï¼ˆéƒ¡ï¼‰']} ç·¯åº¦çµŒåº¦({row['ä½æ‰€_ç·¯åº¦']},{row['ä½æ‰€_çµŒåº¦']}) é€£çµ¡å…ˆ_FormURL:{row['é€£çµ¡å…ˆ_FormURL']}")

    return row["ã‚¨ãƒ©ãƒ¼"]


def main(i, prefecture, argv):
    """
    usage: python main.py               # output csv files (default)
    usage: python main.py --output-json # output json files
    """

    prefecture_number_str = str(i).zfill(2)
    prefecture_name = get_prefecture_name(prefecture)
    logger.info(f"PREFECTURE_NUMBER {i}: {prefecture_name} ({prefecture})")

    try:
        opendata_files = os.listdir(f"./data_files/shinryoujo_{i}")
        opendata_file = opendata_files[0]

        file_path = f"./data_files/shinryoujo_{i}/{opendata_file}"
        with pdfplumber.open(file_path) as pdf:
            first_page = pdf.pages[0]
            first_table = first_page.extract_table()
            if first_table is None or len(first_table) < 2:
                logger.warning("No table found.")
                return
            headers, data = get_first_page(first_table, prefecture_name)
            df = pd.DataFrame(data, columns=headers)

            for page_num in range(1, len(pdf.pages)):
                page = pdf.pages[page_num]
                table = page.extract_table()
                if table:
                    page_df = pd.DataFrame(table, columns=headers)

                    # ã€ŒåŸºæœ¬æƒ…å ±ã€ã€Œæ–½è¨­åã€ã€ŒåŒ»ç™‚æ©Ÿé–¢åã€ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤
                    page_df = fix_format_page_df(page_df, 1)
                    df = pd.concat([df, page_df], ignore_index=True)

        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å¤‰æ›/çµ±ä¸€
        df = unify_column_names(df, prefecture_name)

        # ãƒ‡ãƒ¼ã‚¿ã®æ•´ç†
        df = clear_change_line(df)

        # è¿½åŠ åˆ—ã®åˆæœŸåŒ–
        df["ã‚¨ãƒ©ãƒ¼"] = ""
        df["ä½æ‰€_éƒ½é“åºœçœŒ"] = ""
        df["ä½æ‰€_å¸‚åŒºç”ºæ‘ï¼ˆéƒ¡ï¼‰"] = ""
        df["æ–½è¨­_å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"] = ""
        df["ä½æ‰€_ç·¯åº¦"], df["ä½æ‰€_çµŒåº¦"] = 0, 0

        # æ²–ç¸„çœŒã¨é™å²¡çœŒã¯ã€å…¬è¡¨ã®å¸Œæœ›ã®æœ‰ç„¡ã€ã®åˆ—ã‚’å‰Šé™¤
        if prefecture_name in ["æ²–ç¸„çœŒ", "é™å²¡çœŒ"]:
            df.drop(df.columns[0], axis=1, inplace=True)

        if "é€£çµ¡å…ˆ_éƒµä¾¿ç•ªå·" in df.columns:
            # éƒµä¾¿ç•ªå·ã‹ã‚‰å¸‚åŒºç”ºæ‘ã‚’å–å¾—
            df["ä½æ‰€_éƒ½é“åºœçœŒ"], df["ä½æ‰€_å¸‚åŒºç”ºæ‘ï¼ˆéƒ¡ï¼‰"], df["æ–½è¨­_å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"] = zip(
                *df["é€£çµ¡å…ˆ_éƒµä¾¿ç•ªå·"].apply(lambda x: postal2location(x) if pd.notna(x) else ("", "", "")))

            # ã‚¨ãƒ©ãƒ¼åˆ—ã‚’æ›´æ–°ã—ã€éƒµä¾¿ç•ªå·ã‚¨ãƒ©ãƒ¼ã‚’è¿½åŠ ã™ã‚‹
            df["ã‚¨ãƒ©ãƒ¼"] = df.apply(lambda x: add_error_message(
                x, ERROR_LIST.POST_CODE.value) if x["ä½æ‰€_éƒ½é“åºœçœŒ"] == "" and x["ä½æ‰€_å¸‚åŒºç”ºæ‘ï¼ˆéƒ¡ï¼‰"] == "" else x["ã‚¨ãƒ©ãƒ¼"], axis=1)

        if "é€£çµ¡å…ˆ_ä½æ‰€" in df.columns:
            # ä½æ‰€ã«éƒ½é“åºœçœŒãŒæ›¸ã„ã¦ã„ãªã„è¡Œã«prefecture_nameã‚’å…ˆé ­ã«å…¥ã‚Œã‚‹
            null_prefecture_address = df[df["é€£çµ¡å…ˆ_ä½æ‰€"].str.contains(
                prefecture_name) == False]
            if not null_prefecture_address.empty:
                df.loc[null_prefecture_address.index,
                       "é€£çµ¡å…ˆ_ä½æ‰€"] = prefecture_name + null_prefecture_address["é€£çµ¡å…ˆ_ä½æ‰€"]

            # ç·¯åº¦çµŒåº¦ã‚’å–å¾—
            # address_to_coordinates
            df["ä½æ‰€_ç·¯åº¦"], df["ä½æ‰€_çµŒåº¦"] = zip(
                *df["é€£çµ¡å…ˆ_ä½æ‰€"].apply(lambda x: address_to_coordinates(x) if pd.notna(x) else ("0", "0")))

            # ã‚¨ãƒ©ãƒ¼
            df["ã‚¨ãƒ©ãƒ¼"] = df.apply(
                lambda x: add_error_message(x, ERROR_LIST.LAT_LON_ERROR.value) if x["ä½æ‰€_ç·¯åº¦"] == "0" and x["ä½æ‰€_çµŒåº¦"] == "0" else x["ã‚¨ãƒ©ãƒ¼"], axis=1)

        if "é€£çµ¡å…ˆ_FormURL" in df.columns:
            # URLãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª
            df["ã‚¨ãƒ©ãƒ¼"] = df.apply(
                lambda x: add_error_message(x, ERROR_LIST.URL_INVALID.value)
                if x["é€£çµ¡å…ˆ_FormURL"] != '' and not is_valid_url(x["é€£çµ¡å…ˆ_FormURL"])
                else x["ã‚¨ãƒ©ãƒ¼"],
                axis=1
            )

            df["ã‚¨ãƒ©ãƒ¼"] = df.apply(
                lambda x: add_error_message(x, ERROR_LIST.URL_EXPIRED.value)
                if x["é€£çµ¡å…ˆ_FormURL"] != '' and not is_accessible_url(x["é€£çµ¡å…ˆ_FormURL"])
                else x["ã‚¨ãƒ©ãƒ¼"],
                axis=1
            )

        # åˆ—ã‚’ä¸¦ã³æ›¿ãˆã‚‹
        df = reorder_columns(df)

        # GeoJSONã§ç·¯åº¦çµŒåº¦ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹
        df["ã‚¨ãƒ©ãƒ¼"] = df.apply(check_location_in_japan, axis=1)

        if argv[0] == '--output-json':
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
            prefecture_number_str = str(i).zfill(2)
            output_file_path = f"./output_files/json/{prefecture_number_str}_{prefecture}.json"
            df.to_json(output_file_path, orient='records',
                       force_ascii=False)
        else:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
            prefecture_number_str = str(i).zfill(2)
            output_file_path = f"./output_files/{prefecture_number_str}_{prefecture}.csv"
            df.to_csv(output_file_path, header=True, index=False)

    except Exception as e:
        logger.error(e)


# ==================================================================
# Parameters
# ==================================================================

# ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ
class ERROR_LIST(Enum):
    POST_CODE = "éƒµä¾¿ç•ªå·"
    # å€‹åˆ¥äº‹æ¥­æ‰€ãƒ‡ãƒ¼ã‚¿ã¨åŒ»ç™‚æ©Ÿé–¢ãƒ‡ãƒ¼ã‚¿ã«éƒµä¾¿ç•ªå·ãŒãªã„

    LAT_LON_ERROR = "ç·¯åº¦çµŒåº¦(æ¤œç´¢ã‚¨ãƒ©ãƒ¼)"
    # ç·¯åº¦çµŒåº¦ã§geojsonã‹ã‚‰éƒ½é“åºœçœŒã‚’æŠ½å‡ºã—ãŸéš›ã«ã€ç©ºæ¬„ã«ãªã£ãŸã®ã‹éƒµä¾¿ç•ªå·ã‹ã‚‰éƒ½é“åºœçœŒåã¨æ¯”è¼ƒã—ã¦ä¸ä¸€è‡´

    LAT_LON_MISMATCH = "ç·¯åº¦çµŒåº¦(ä¸ä¸€è‡´)"
    # ã‚»ãƒ³ã‚¿ãƒ¼ã‹ã‚‰å–å¾—ã—ãŸç·¯åº¦çµŒåº¦ã¨Google APIã®ç·¯åº¦çµŒåº¦ã¨æ¯”è¼ƒã—ã€åˆã‚ãªã„

    GOOGLE_LAT_LON_ERROR = "ç·¯åº¦çµŒåº¦(Googleã‚¨ãƒ©ãƒ¼)"
    # Google APIã§ç·¯åº¦çµŒåº¦ã‚’å–å¾—ã™ã‚‹éš›ã€è¿”å´å€¤ãŒãªã„å ´åˆã‚„2å€¤ä»¥ä¸Šã®è¿”å´å€¤ãŒã‚ã‚‹

    URL_INVALID = "URL(ç„¡åŠ¹ãªURL)"
    # ç”£ç§‘ã€å©¦äººç§‘åˆã¯ç”£å©¦äººç§‘ã®æ¨™æ¦œã®æœ‰ç„¡ã®ã‚«ãƒ©ãƒ å†…ã«pã‚„mãŒå…¥ã‚Šè¾¼ã‚“ã§ã„ã‚‹(?)

    URL_EXPIRED = "URL(ãƒªãƒ³ã‚¯åˆ‡ã‚Œ)"
    # URLã«Pingã‚’æ‰“ã¡ã€404ãªã©ã®ã‚¨ãƒ©ãƒ¼ãŒè¿”ã£ãŸ


# ==================================================================
# Main
# ==================================================================
if __name__ == "__main__":
    try:
        # ãƒ•ã‚©ãƒ«ãƒ€åˆæœŸåŒ–
        if not os.path.exists("./output_files"):
            os.mkdir("./output_files")
        if not os.path.exists("./output_files/json"):
            os.mkdir("./output_files/json")
        if not os.path.exists("./logs"):
            os.mkdir("./logs")

        # ãƒ­ã‚°è¨­å®š
        # yyyymmdd format
        current_date = datetime.now().strftime("%Y%m%d")
        basicConfig(filename=f"logs/{current_date}.log", filemode='a',
                    format='[%(asctime)s]%(levelname)-7s: %(message)s',
                    level=INFO)

        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ï¼ˆæ¨™æº–å‡ºåŠ›ï¼‰ã«å‡ºåŠ›ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©ã‚’ä½œæˆ
        logger = logging.getLogger(__name__)
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        logger.addHandler(console_handler)

        # start
        logger.info(f"{'='*10} START {'='*10}")

        # load config
        logger.info("Initializing jageocoder...")

        # geojson
        geojson_file_path = "./data_files/JPGIS2014/N03-20240101_prefecture.geojson"
        gdf = gpd.read_file(geojson_file_path)  # GeoJSONãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€

        # jageocoder
        jageocoder.init(url='https://jageocoder.info-proto.com/jsonrpc')

        # éƒ½é“åºœçœŒã®è¾æ›¸ã¨è‹±èªåã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€
        prefectures, PREFECTURES = load_prefectures('./table/prefectures.csv')

        # csvå‡ºåŠ›ãƒ•ã‚©ãƒãƒ¼ãƒ‰+è»¢æ›ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€
        output_format_list_df = pd.read_csv(
            './table/output_format.csv', header=0, dtype=str)

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€éƒµä¾¿ç•ªå·ã‚’ã‚­ãƒ¼ã€å¸‚åŒºç”ºæ‘åã‚’å€¤ã¨ã™ã‚‹è¾æ›¸ã‚’ä½œæˆ
        address_df = pd.read_csv(
            './data_files/ken_all/utf_ken_all.csv', header=None, dtype=str,
            names=["jis", "old", "postal", "prefecture_kana", "city_kana", "town_kana", "prefecture", "city", "town", "multi", "koaza", "block", "double", "update", "reason"])

        # äº‹æ¥­æ‰€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        # æ–‡å­—ã‚³ãƒ¼ãƒ‰ã«Shift-JISã§ãªã„ã‚‚ã®ã‚‚æ··ã˜ã£ã¦ã„ã‚‹ã‚ˆã†ã§ã€ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã™ã‚‹
        with codecs.open('./data_files/jigyosyo/JIGYOSYO.CSV', "r", "Shift-JIS", "ignore") as file:
            # ã‚«ãƒ©ãƒ åã¯ken_allã¨åˆã‚ã›ã‚‹
            jigyosyo_df = pd.read_csv(
                file, header=None, dtype=str, encoding="shift-jis",
                names=["jis", "jigyosyo_kana", "jigyosyo", "prefecture", "city", "town", "detail", "postal", "old", "branch", "type", "multi", "diff"])

        # å‡ºåŠ›ç¢ºèª
        argv = sys.argv[1:] or ['']
        if argv[0] == '--output-json':
            logger.info("Exporting to JSON files...")
        else:
            logger.info("Exporting to CSV files...")

        # é€šå¸¸å‡¦ç†
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ --test ã¯åŒ—æµ·é“ã ã‘å®Ÿè¡Œ
        for index, prefecture in enumerate(PREFECTURES[0:], 1):
            if "--test" in argv:
                if index == 1:
                    main(index, prefecture, argv)
                break
            else:
                main(index, prefecture, argv)

    except Exception as e:
        print(f"Error: {e}")

    finally:
        logger.info(f"{'='*10}  END  {'='*10}")
